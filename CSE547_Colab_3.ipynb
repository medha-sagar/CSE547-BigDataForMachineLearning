{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSE547 - Colab 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/medha-sagar/CSE547-BigDataForMachineLearning/blob/master/CSE547_Colab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPt5q27L5557",
        "colab_type": "text"
      },
      "source": [
        "# CSE547 - Colab 3\n",
        "## K-Means & PCA\n",
        "\n",
        "Adapted From Stanford's CS246"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0-YhEpP_Ds-",
        "colab_type": "text"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zsj5WYpR9QId",
        "colab_type": "text"
      },
      "source": [
        "Let's setup Spark on your Colab environment.  Run the cell below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qHai2252mI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "53a0bff1-4d81-4549-cfc1-c5f4a981b159"
      },
      "source": [
        "!pip install pyspark\n",
        "!pip install -U -q PyDrive\n",
        "!apt install openjdk-8-jdk-headless -qq\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/5a/271c416c1c2185b6cb0151b29a91fff6fcaed80173c8584ff6d20e46b465/pyspark-2.4.5.tar.gz (217.8MB)\n",
            "\u001b[K     |████████████████████████████████| 217.8MB 63kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/53/c737818eb9a7dc32a7cd4f1396e787bd94200c3997c72c1dbe028587bd76/py4j-0.10.7-py2.py3-none-any.whl (197kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 50.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-2.4.5-py2.py3-none-any.whl size=218257927 sha256=44379f158e1ebcaf1e5fc0ddd3cedc7fb002116886774f8a5ab7719497347bd9\n",
            "  Stored in directory: /root/.cache/pip/wheels/bf/db/04/61d66a5939364e756eb1c1be4ec5bdce6e04047fc7929a3c3c\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.7 pyspark-2.4.5\n",
            "openjdk-8-jdk-headless is already the newest version (8u242-b08-0ubuntu3~18.04).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 25 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwtlO4_m_LbQ",
        "colab_type": "text"
      },
      "source": [
        "Now we import some of the libraries usually needed by our workload.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twk-K-jilWK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark import SparkContext, SparkConf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BtrJlMBt1Ela"
      },
      "source": [
        "Let's initialize the Spark context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Vm3sAVeK1EDZ",
        "colab": {}
      },
      "source": [
        "# create the session\n",
        "conf = SparkConf().set(\"spark.ui.port\", \"4050\")\n",
        "\n",
        "# create the context\n",
        "sc = pyspark.SparkContext(conf=conf)\n",
        "spark = SparkSession.builder.getOrCreate()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YqovskkH1DmC"
      },
      "source": [
        "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DueQggJc1DDk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "outputId": "fb21f31e-a173-48e6-b949-7ce4c08378b9"
      },
      "source": [
        "spark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://8c94103f49a1:4050\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.4.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f7c38800c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Iid7lXcG1CY8"
      },
      "source": [
        "If you are running this Colab on the Google hosted runtime, the cell below will create a *ngrok* tunnel which will allow you to still check the Spark UI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sDnGLVPH1BPQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "9166f53d-7b51-4271-9872-a7ecb6767147"
      },
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 4050 &')\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-04-17 07:55:02--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 3.214.203.143, 52.2.164.113, 52.4.202.19, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|3.214.203.143|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13773305 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.13M  40.8MB/s    in 0.3s    \n",
            "\n",
            "2020-04-17 07:55:03 (40.8 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13773305/13773305]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "http://ae60b74f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAYRX2PMm0L6",
        "colab_type": "text"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXdMR6wnEIM",
        "colab_type": "text"
      },
      "source": [
        "In this Colab, rather than downloading a file from Google Drive, we will load a famous machine learning dataset, the [Breast Cancer Wisconsin dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html), using the ```scikit-learn``` datasets loader."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5K93ABEy9Zlo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "breast_cancer = load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpsaYOqRxar2",
        "colab_type": "text"
      },
      "source": [
        "For convenience, given that the dataset is small, we first construct a Pandas dataframe, tune the schema, and then convert it into a Spark dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oitav_xhQD9w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "96a6e66c-397d-4559-9c27-72ab09f1736c"
      },
      "source": [
        "pd_df = pd.DataFrame(breast_cancer.data, columns=breast_cancer.feature_names)\n",
        "df = spark.createDataFrame(pd_df)\n",
        "\n",
        "def set_df_columns_nullable(spark, df, column_list, nullable=False):\n",
        "    for struct_field in df.schema:\n",
        "        if struct_field.name in column_list:\n",
        "            struct_field.nullable = nullable\n",
        "    df_mod = spark.createDataFrame(df.rdd, df.schema)\n",
        "    return df_mod\n",
        "\n",
        "df = set_df_columns_nullable(spark, df, df.columns)\n",
        "df = df.withColumn('features', array(df.columns))\n",
        "vectors = df.rdd.map(lambda row: Vectors.dense(row.features))\n",
        "\n",
        "df.printSchema()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- mean radius: double (nullable = false)\n",
            " |-- mean texture: double (nullable = false)\n",
            " |-- mean perimeter: double (nullable = false)\n",
            " |-- mean area: double (nullable = false)\n",
            " |-- mean smoothness: double (nullable = false)\n",
            " |-- mean compactness: double (nullable = false)\n",
            " |-- mean concavity: double (nullable = false)\n",
            " |-- mean concave points: double (nullable = false)\n",
            " |-- mean symmetry: double (nullable = false)\n",
            " |-- mean fractal dimension: double (nullable = false)\n",
            " |-- radius error: double (nullable = false)\n",
            " |-- texture error: double (nullable = false)\n",
            " |-- perimeter error: double (nullable = false)\n",
            " |-- area error: double (nullable = false)\n",
            " |-- smoothness error: double (nullable = false)\n",
            " |-- compactness error: double (nullable = false)\n",
            " |-- concavity error: double (nullable = false)\n",
            " |-- concave points error: double (nullable = false)\n",
            " |-- symmetry error: double (nullable = false)\n",
            " |-- fractal dimension error: double (nullable = false)\n",
            " |-- worst radius: double (nullable = false)\n",
            " |-- worst texture: double (nullable = false)\n",
            " |-- worst perimeter: double (nullable = false)\n",
            " |-- worst area: double (nullable = false)\n",
            " |-- worst smoothness: double (nullable = false)\n",
            " |-- worst compactness: double (nullable = false)\n",
            " |-- worst concavity: double (nullable = false)\n",
            " |-- worst concave points: double (nullable = false)\n",
            " |-- worst symmetry: double (nullable = false)\n",
            " |-- worst fractal dimension: double (nullable = false)\n",
            " |-- features: array (nullable = false)\n",
            " |    |-- element: double (containsNull = false)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtR1xRvonxiO",
        "colab_type": "text"
      },
      "source": [
        "With the next cell, we build the two datastructures that we will be using throughout this Colab:\n",
        "\n",
        "\n",
        "*   ```features```, a dataframe of Dense vectors, containing all the original features in the dataset;\n",
        "*   ```labels```, a series of binary labels indicating if the corresponding set of features belongs to a subject with breast cancer, or not.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GP23Xkgwi0SD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pyspark.ml.linalg import Vectors\n",
        "features = spark.createDataFrame(vectors.map(Row), [\"features\"])\n",
        "labels = pd.Series(breast_cancer.target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRaF2A_j_nC7",
        "colab_type": "text"
      },
      "source": [
        "### Your task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebLNUxP0_8x3",
        "colab_type": "text"
      },
      "source": [
        "If you run successfully the Setup and Data Preprocessing stages, you are now ready to cluster the data with the [K-means](https://spark.apache.org/docs/latest/ml-clustering.html) algorithm included in MLlib (Spark's Machine Learning library).\n",
        "Set the ```k``` parameter to **2**, use a seed of **1**, fit the model, and the compute the [Silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)) (i.e., a measure of quality of the obtained clustering).  \n",
        "\n",
        "**IMPORTANT:** use the MLlib implementation of the Silhouette score (via ```ClusteringEvaluator```)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xVIfPHZwWaE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 323
        },
        "outputId": "37d513b9-4169-4ee2-cf63-443bb85bf18b"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "from pyspark.ml.evaluation import ClusteringEvaluator\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(features)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(features)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.8342904262826145\n",
            "Cluster Centers: \n",
            "[1.25562991e+01 1.85703653e+01 8.11234703e+01 4.96061872e+02\n",
            " 9.48844977e-02 9.10998174e-02 6.24377642e-02 3.34325434e-02\n",
            " 1.78057991e-01 6.34540183e-02 3.04190868e-01 1.21515320e+00\n",
            " 2.15288059e+00 2.37852922e+01 7.17326256e-03 2.34746895e-02\n",
            " 2.87455128e-02 1.06363242e-02 2.06135799e-02 3.74750297e-03\n",
            " 1.40439018e+01 2.47095434e+01 9.19375114e+01 6.19647945e+02\n",
            " 1.29959110e-01 2.23311758e-01 2.19214947e-01 9.13298425e-02\n",
            " 2.83553653e-01 8.32819406e-02]\n",
            "[1.93799237e+01 2.16945802e+01 1.28231298e+02 1.18592977e+03\n",
            " 1.01294580e-01 1.48612977e-01 1.76939466e-01 1.00698779e-01\n",
            " 1.91539695e-01 6.06029008e-02 7.42803817e-01 1.22253817e+00\n",
            " 5.25058015e+00 9.56781679e+01 6.59868702e-03 3.21766947e-02\n",
            " 4.24197710e-02 1.56739847e-02 2.03039695e-02 3.95338931e-03\n",
            " 2.37094656e+01 2.89126718e+01 1.58496183e+02 1.75302290e+03\n",
            " 1.40424733e-01 3.57757710e-01 4.49306107e-01 1.92431069e-01\n",
            " 3.11881679e-01 8.61654962e-02]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzZn9ulSM_UJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "f0dc1042-e223-4174-e526-7189124ac669"
      },
      "source": [
        "pred = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()\n",
        "pd.Series(pred)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "564    1\n",
              "565    1\n",
              "566    0\n",
              "567    1\n",
              "568    0\n",
              "Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rU_stMCnRnaF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "db597b75-54a6-4d2f-c00f-54b0ca9435da"
      },
      "source": [
        "pd.crosstab(pd.Series(pred),labels)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82</td>\n",
              "      <td>356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    0    1\n",
              "row_0          \n",
              "0       82  356\n",
              "1      130    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GB09n7sqTO6",
        "colab_type": "text"
      },
      "source": [
        "Take the predictions produced by K-means, and compare them with the ```labels``` variable (i.e., the ground truth from our dataset).  \n",
        "\n",
        "Compute how many data points in the dataset have been clustered correctly (i.e., positive cases in one cluster, negative cases in the other).\n",
        "\n",
        "*HINT*: you can use ```np.count_nonzero(series_a == series_b)``` to quickly compute the element-wise comparison of two series.\n",
        "\n",
        "**IMPORTANT**: K-means is a clustering algorithm, so it will not output a label for each data point, but just a cluster identifier!  As such, label ```0``` does not necessarily match the cluster identifier ```0```.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQhC3APIPPM5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# YOUR CODE HERE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLIprM1JsdTU",
        "colab_type": "text"
      },
      "source": [
        "Now perform dimensionality reduction on the ```features``` using the [PCA](https://spark.apache.org/docs/latest/ml-features.html#pca) statistical procedure, available as well in MLlib.\n",
        "\n",
        "Set the ```k``` parameter to **2**, effectively reducing the dataset size of a **15X** factor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p4J8JMDkSb24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2985374f-b9dc-43a2-d19a-8d2085577f14"
      },
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "\n",
        "pca = PCA(k=2, inputCol=\"features\", outputCol=\"pcaFeatures\")\n",
        "model = pca.fit(features)\n",
        "\n",
        "#result = model.transform(features).select(\"pcaFeatures\").toDF()\n",
        "result = model.transform(features).select(\"pcaFeatures\")\n",
        "result.show(4)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------------+\n",
            "|         pcaFeatures|\n",
            "+--------------------+\n",
            "|[-2260.0138862925...|\n",
            "|[-2368.9937557820...|\n",
            "|[-2095.6652015478...|\n",
            "|[-692.69051005705...|\n",
            "+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07Ku9BOza6e7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#result.columns = [\"features\"]\n",
        "result =result.withColumnRenamed(\"pcaFeatures\", \"features\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8leQR4-atMAl",
        "colab_type": "text"
      },
      "source": [
        "Now run K-means with the same parameters as above, but on the ```pcaFeatures``` produced by the PCA reduction you just executed. (Again with a seed of **1**)\n",
        "\n",
        "Compute the Silhouette score, as well as the number of data points that have been clustered correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_snSSj5k2y5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "6d4da63a-8169-4ebd-d2b2-786bdf3b3d59"
      },
      "source": [
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(result)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.transform(result)\n",
        "\n",
        "# Evaluate clustering by computing Silhouette score\n",
        "evaluator = ClusteringEvaluator()\n",
        "\n",
        "silhouette = evaluator.evaluate(predictions)\n",
        "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Silhouette with squared euclidean distance = 0.8348610363444837\n",
            "Cluster Centers: \n",
            "[-793.36109344  105.48246852]\n",
            "[-2124.69173996   107.54465437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMUb_afmlIEW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "acf47a59-3eb6-4160-91b9-d5d6c9351791"
      },
      "source": [
        "pred = predictions.select('prediction').rdd.flatMap(lambda x: x).collect()\n",
        "pd.Series(pred)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      1\n",
              "1      1\n",
              "2      1\n",
              "3      0\n",
              "4      1\n",
              "      ..\n",
              "564    1\n",
              "565    1\n",
              "566    0\n",
              "567    1\n",
              "568    0\n",
              "Length: 569, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0tgVBLIjhPzL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "94a15179-cb9c-434f-e7af-9ff53d9c0715"
      },
      "source": [
        "pd.crosstab(pd.Series(pred),labels)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>col_0</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>row_0</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>82</td>\n",
              "      <td>356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>130</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "col_0    0    1\n",
              "row_0          \n",
              "0       82  356\n",
              "1      130    1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIrXJyVNP2AI",
        "colab_type": "text"
      },
      "source": [
        "Once you obtained the desired results, **head over to Gradescope and submit your solution for this Colab**!"
      ]
    }
  ]
}